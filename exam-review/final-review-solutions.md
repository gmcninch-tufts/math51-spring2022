---
title: |
  Math 51 Spring 2022 - Final Exam - some review problems **Solutions**
date: 2022-05-01
fontsize: 12pt
header-includes: |
  \usepackage[top=10mm,bottom=20mm,left=20mm,right=20mm]{geometry}
  \usepackage{mathrsfs}
---


#. Indicate which of the following best represents a
   *simplified guess* for a particular solution $p(t)$ to the
   non-homogeneous linear ODE: $$(D-3)(D-1)x = te^{3t} + \cos(2t)$$

   a. $p(t) = k_1 te^{3t} + k_2 \cos(2t) + k_3 \sin(2t)$
   
   b. $p(t) = k_1 te^{3t} + k_2 \cos(2t)$
   
   c. $p(t) = k_1 te^{3t} + k_2 t^2 e^{3t} + k_3 \cos(2t)$
   
   d. $p(t) = k_1 te^{3t} + k_2 t^2 e^{3t} + k_3 \cos(2t) + k_4 \sin(2t)$
 

   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was d.
   
   -----
   
   \color{black}
   :::


#. Indicate which of the following represents the general
   solution to the homogeneous linear ODE $(D^2 - 2D + 2)^2x = 0.$

   a. $h(t) = c_1 e^{-t}\cos(t) + c_2 e^{-t}\sin(t) + c_3 te^{-t}\cos(t) + c_4
      te^{-t}\sin(t)$
   
   b. $h(t) = c_1 e^t\cos(t) + c_2 e^t\sin(t)$

   c. $h(t) = c_1 e^t\cos(t) + c_2 e^t\sin(t) + c_3 te^t\cos(t) + c_4
      te^t\sin(t)$  

   d. $h(t) = c_1 te^t\cos(t) + c_2 te^t\sin(t) + c_3 t^2e^t\cos(t) + c_4 t^2e^t\sin(t)$

   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was ~~a.~~ c. 
   
   (The roots of the char poly are $\lambda = 1 \pm i$, which leads to solutions of the form
   $t^j \sin(t)$ and $t^j \cos(t)$.)
   
   -----
   
   \color{black}
   :::


#. The matrix $A = \begin{bmatrix} -2 & 5 \\ -2 & 4 \end{bmatrix}$
   has characteristic polynomial $\lambda^2 - 2\lambda + 2$ and thus
   its eigenvalues are $\lambda = 1 + i$ and $\lambda = 1-i$.
   
   Which of the following is an eigenvector for $A$?
   
   a. $A$ has no eigenvectors.
  
   b. $\begin{bmatrix} 3 - i \\ 2 \end{bmatrix}$  

   c. $\begin{bmatrix} 2 \\ -3 + i \end{bmatrix}$
   
   d. $\begin{bmatrix} 3 + i \\ 2 \end{bmatrix}$

   :::{.solution}
   \color{red}
   **Solution:**
      
   both b and d give eigenvectors. You can check that 
   $\begin{bmatrix} -2 & 5 \\ -2 & 4 \end{bmatrix} \cdot \begin{bmatrix} 3 \pm i \\ 2 \end{bmatrix}
   = (1\pm i)\begin{bmatrix} 3 \pm i \\ 2 \end{bmatrix}$
   
   -----
   
   \color{black}
   :::



#. Consider the linear system of ODEs $$(\diamondsuit) \quad D \mathbf{x} =
   \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 2 & 1 & 5 \end{bmatrix}
   \mathbf{x} + \begin{bmatrix} 0 \\ 0 \\ e^t \end{bmatrix}.$$
   
   A third order linear ODE is *equivalent* to this system if for each of
   its solutions $x(t)$, the vector-valued function $\mathbf{x}(t) =
   \begin{bmatrix} x(t) \\ x'(t) \\ x''(t) \end{bmatrix}$ is a solution to
   $(\diamondsuit)$. Which of the following linear ODEs is equivalent
   to $(\diamondsuit)$?
   
   a. $(D^3 - 2D^2 - D - 5)x = e^t$
   
   b. $(D^3 - 5D^2 - D - 2)x = e^t$
   
   c. $(D^3 + 2D^2 + D + 5)x = -e^t$
   
   d. $(D^3 + 5D^2 + D + 2)x = -e^t$

   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was b.
   
   -----
   
   \color{black}
   :::


#. Let $A = \begin{bmatrix} 2 & 0 & 2 \\ 0 & -1 & 1 \\ 0 & 0
   & 2\end{bmatrix}$.  $\lambda = 2$ is an eigenvalue of $A$ with
   multiplicity two.  The matrix $A - 2\mathbf{I}_3$ satisfies
   $(A-2\mathbf{I}_3)^2 = \begin{bmatrix} 0 & 0 & 2 \\ 0 & -3 & 1 \\ 0
   & 0 & 0 \end{bmatrix}^2 = \begin{bmatrix} 0 & 0 & 0 \\ 0 & 9 & -3
   \\ 0 & 0 & 0 \end{bmatrix} \sim \begin{bmatrix} 0 & 3 & -1 \\ 0 & 0
   & 0 \\ 0 & 0 & 0 \end{bmatrix}.$ Thus the generalized eigenvectors
   of $A$ for $\lambda = 2$ are generated by $\mathbf{v} =
   \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$ and $\mathbf{w} =
   \begin{bmatrix} 0 \\ 1 \\ 3 \end{bmatrix}$.
   
   Which of the following represents a solution $\mathbf{h}(t)$ to the
   system $D \mathbf{x} = A \mathbf{x}$ with the property that
   $\mathbf{h}(0) = \begin{bmatrix} 1 \\ 2 \\ 6 \end{bmatrix}$?
   
   a. $\mathbf{h}(t) = e^{2t} \begin{bmatrix} 1 \\ 2 \\ 6 \end{bmatrix}$
   
   b. $\mathbf{h}(t) =  e^{2t} \begin{bmatrix} 1+ 12t \\ 2 \\ 6 \end{bmatrix}$

   c. $\mathbf{h}(t) =  e^{2t} \begin{bmatrix} 1+ t \\ 2 \\ 6 \end{bmatrix}$

   d. No solution $\mathbf{h}(t)$ has the property that
      $\mathbf{h}(0) =    \begin{bmatrix} 1 \\ 2 \\ 6 \end{bmatrix}$.

   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was b.
   
   -----
   
   \color{black}
   :::



#. Consider the homogeneous system $(\diamondsuit) \quad D
   \mathbf{x} = A \mathbf{x}$ where $A$ is a $3 \times 3$ matrix, and
   let $\mathbf{h}_1(t),\mathbf{h}_2(t)$ be solutions
   to $(\diamondsuit)$. Which of the following statements is correct?
   
   a. $\mathbf{h}_1(0)$ and $\mathbf{h}_2(0)$ are *eigenvectors* for
      $A$.
   
   b. The system $(\diamondsuit)$ has exactly two solutions.

   c. If the vectors $\mathbf{h}_1(0),\mathbf{h}_2(0)$
      are linearly independent, then the general solution to
      $(\diamondsuit)$ is given by $\mathbf{x}(t) =
      c_1\mathbf{h}_1(t) + c_2\mathbf{h}_2(t)$.
   
   d. None of the above statements is correct.


   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was d.
   
   To see that **a.** is incorrect, consider solutions $e^{\lambda t} \mathbf{v}$
   and $e^{\mu t} \mathbf{w}$ arising from eigenvectors $\mathbf{v}$ and $\mathbf{w}$.
   
   Then there is a  solution $h(t) = e^{\lambda t} \mathbf{v} + e^{\mu t} \mathbf{w}$
   but $h(0) = \mathbf{v} + \mathbf{w}$ which is not an eigenvector if $\lambda \ne \mu$.
   
   **b.** in incorrect. Indeed, all linear combinations $c_1\mathbf{h}_1(t) + c_2 \mathbf{h}_2(t)$ are
   solutions, so there are always *infinitely many solutions.*
   
   Finally, **c.** is incorrect because for a $3 \times 3$ system the
   general solution is generated by three solutions with linearly
   independent initial vectors; two solutions are not enough.
   
   -----
   
   \color{black}
   :::

#. The matrix $A = \begin{bmatrix} 1 & 1 \\ 2 & 2
   \end{bmatrix}$ has characteristic polynomial $\lambda(\lambda - 3)$
   and hence has eigenvalues $\lambda = 0$ and $\lambda = 3$. An
   eigenvector for $\lambda =0$ is given by $\mathbf{v} = \begin{bmatrix} -1 \\
   1 \end{bmatrix}$ and an eigenvector for $\lambda = 3$ is
   given by $\mathbf{w} = \begin{bmatrix} 1 \\ 2
   \end{bmatrix}$.
   

   Find a particular solution $\mathbf{p}(t)$ for the system of linear
   ODEs $$D \mathbf{x} = A \mathbf{x} + \begin{bmatrix} 0 \\ t
   \end{bmatrix}.$$

   :::{.solution}
   \color{red}
   **Solution:**

   The general solution is generated by the solutions obtained from
   eigenvectors: $$\mathbf{h}_1(t) = e^{0t}\begin{bmatrix} -1 \\ 1
   \end{bmatrix} = \begin{bmatrix} -1 \\ 1 \end{bmatrix} \quad
   \text{and} \quad \mathbf{h}_2(t) = e^{3t}\begin{bmatrix}1 \\ 2
   \end{bmatrix}$$
   
   To find a particular solution, form the Wronskian matrix
   $$W = \begin{bmatrix} -1 & e^{3t} \\
   1 & 2e^{3t} \end{bmatrix}$$
   and notice that
   $\det W = -3e^{3t}$.
   
   A particular solution has the form $\mathbf{p}(t) = c_1(t)
   \mathbf{h}_1(t) + c_2(t) \mathbf{h}_2(t)$, where the vector
   $\mathbf{c} = \begin{bmatrix} c_1(t) \\ c_2(t) \end{bmatrix}$ satisfies
   the matrix equations
   $$W \mathbf{c}' = \begin{bmatrix} 0 \\ t \end{bmatrix}.$$
   
   Using Cramer's Rule, we find that
   $$c_1'(t) = \dfrac{\det \begin{bmatrix} 0 & e^{3t} \\
   t & 2e^{3t} \end{bmatrix}}{-3e^{3t}} = \dfrac{-te^{3t}}{-3e^{3t}} = \dfrac{t}{3}.$$

   $$c_2'(t) = \dfrac{\det \begin{bmatrix} -1 & 0 \\
   1 & t \end{bmatrix}}{-3e^{3t}} = \dfrac{-t}{-3e^{3t}}= \dfrac{te^{-3t}}{3}$$

   Now we integrate to find $c_1(t)$ and $c_2(t)$:
   
   $$c_1(t) = \int c_1'(t) dt = \dfrac{1}{3} \int t dt = \dfrac{t^2}{6} + A.$$

   For $c_2$ we integrate by parts with $u = t, dv = e^{-3t}dt$: 
   
   $$c_2(t) = \int c_2'(t) dt = \dfrac{1}{3} \int te^{-3t} dt
   = \dfrac{1}{3} \left(\dfrac{-t}{3}e^{-3t} + \dfrac{1}{3} \int e^{-3t}dt \right)
   =\dfrac{-1}{9}e^{-3t}\left(t + \dfrac{1}{3} \right) + B$$

   We may take $A=B=0$ since we only seek a particular solution. This gives
   \begin{align*}
   \mathbf{p}(t) =&
   \dfrac{t^2}{6} \begin{bmatrix} -1 \\ 1
   \end{bmatrix} + 
   \dfrac{-1}{9}e^{-3t}\left(t + \dfrac{1}{3} \right) e^{3t}\begin{bmatrix}1 \\ 2
   \end{bmatrix} \\
   =& \dfrac{t^2}{6} \begin{bmatrix} -1 \\ 1
   \end{bmatrix} + 
   \dfrac{-1}{9}\left(t + \dfrac{1}{3} \right)\begin{bmatrix}1 \\ 2
   \end{bmatrix}
   \end{align*}

   -----
   
   \color{black}
   :::


#. Let $A = \begin{bmatrix} 0 & 1 \\ -5 & 4 \end{bmatrix}$.

   The characteristic polynomial of $A$ is $r^2 -4r +5$ so the eigenvalues
   of $A$ are $\lambda = 2 \pm i$.
   
   Moreover, $\mathbf{v} = \begin{bmatrix} 2-i\\ 5 \end{bmatrix}$ is an 
   eigenvector for $\lambda = 2 + i$.

   a. Find the general solution to $D \mathbf{x} = A \mathbf{x}$.
   

   :::{.solution}
   \color{red}
   **Solution:**
      
   The complex solution to (H) is
   \begin{equation*}
   e^{2t}(\cos t +i \sin t)\begin{bmatrix} 2-i\\5\end{bmatrix}
   =e^{2t}\begin{bmatrix}
   2\cos t+\sin t\\
   5\cos t
   \end{bmatrix}
   +i e^{2t}\begin{bmatrix}
   -\cos t+2\sin t\\
   5\sin t
   \end{bmatrix}
   \end{equation*}
   so the real and imaginary parts of this expression generate the general solution
   \begin{equation*}
   x(t)=C_{1}e^{2t}\begin{bmatrix}
   2\cos t+\sin t\\
   5\cos t
   \end{bmatrix}
   +C_{2 } e^{2t}\begin{bmatrix}
   -\cos t+2\sin t\\
   5\sin t
   \end{bmatrix}.
   \end{equation*}

   
   -----
   
   \color{black}
   :::


   b. Solve the initial value problem $D \mathbf{x} = A \mathbf{x},
      \quad \mathbf{x}(0) = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$.

   :::{.solution}
   \color{red}
   **Solution:**
   
   The value at $t=0$ of the general solution given above is
   \begin{equation*}
   X(0)=C_{1}e^{0}\begin{bmatrix}
   2\cos 0+\sin 0\\
   5\cos 0
   \end{bmatrix}
   +C_{2 } e^{0}\begin{bmatrix}
   -\cos 0+2\sin 0\\
   5\sin 0
   \end{bmatrix}
   =C_{1}\begin{bmatrix}
   2\\
   5
   \end{bmatrix}
   +C_{2}\begin{bmatrix}
   -1\\
   0
   \end{bmatrix};
   \end{equation*}
   setting this equal to the desired initial condition yields the system of equations
   \begin {align*}
   2C_{1}-C_{2}&=1\\
   5C_{1}+0C_{2}&=1
   \end{align*}
   which can be solved by performing row operations on the augmented matrix
   \begin{equation*}
   \begin{bmatrix}
   2&-1&|1\\
   5&0&|1
   \end{bmatrix},
   \end{equation*}
   or by using  Cramer's Rule, or simply by noting that the second equation says
   $C_{1}=\frac{1}{5}$, 
   and substituting into the first equation yields
   $\frac{2}{5}-C_{2}=1$
   or
   $C_{2}=-\frac{3}{5}$.
   
   Thus the desired solution of (H) is
   \begin{equation*}
   X(t)=
   \frac{1}{5}e^{2t}\begin{bmatrix}2\cos t+\sin t\\5\cos t\end{bmatrix}
   -\frac{3}{4}e^{2t}\begin{bmatrix}-\cos t+2\sin t\\5\sin t\end{bmatrix}
   =e^{2t}\begin{bmatrix}\cos t-\sin t\\\cos t-3\sin t\end{bmatrix}.
   \end{equation*}
   
   
   -----
   
   \color{black}
   :::



#. Solve the initial value problem $(4D^2 - 4D + 1)x = 0, \quad x(2) = x'(2) = e$.

   :::{.solution}
   \color{red}
   **Solution:**
      
   The polynomial $4r^2 - 4r + 1$ has root $r=1/2$ with multiplicity 2.
   Thus the general solution is given by
   $$x(t) = c_1e^{t/2}  + c_2 te^{t/2}.$$
   
   Note that 
   \begin{align*}
   x'(t) &= D[x(t)] = \dfrac{c_1}{2}e^{t/2} + c_2e^{t/2}D[t] \\
   &= \dfrac{c_1}{2}e^{t/2} + c_2e^{t/2}(D+1/2)[t] \\
   &= \dfrac{c_1}{2}e^{t/2} + c_2e^{t/2}(1 + t/2)
   \end{align*}
   
   Now, we need
   $$e= x(2) = c_1e + 2c_2e$$
   and
   $$e = x'(2) = \dfrac{1}{2}e c_1 + 2ec_2$$
   
   Thus we must solve the matrix equation
   $$\begin{bmatrix} e & 2e \\ e/2 & 2e \end{bmatrix}
   \begin{bmatrix} c_1 \\ c_2 \end{bmatrix} =
   \begin{bmatrix} e \\ e \end{bmatrix}$$
   
   This can be solved in several ways -- e.g. by row operations on the
   augmented matrix, as follows:
   
   \begin{align*}
   \left[
   \begin{array}{c|c}
   \begin{matrix} e & 2e \\ e/2 & 2e \end{matrix} &
   \begin{matrix} e \\ e \end{matrix}
   \end{array}
   \right]
   \sim 
   \left[
   \begin{array}{c|c}
   \begin{matrix} 1 & 2 \\ 1 & 4 \end{matrix} &
   \begin{matrix} 1 \\ 2 \end{matrix}
   \end{array}
   \right]  
   \sim 
   \left[
   \begin{array}{c|c}
   \begin{matrix} 1 & 2 \\ 0 & 2 \end{matrix} &
   \begin{matrix} 1 \\ 1 \end{matrix}
   \end{array}
   \right]     
   \sim 
   \left[
   \begin{array}{c|c}
   \begin{matrix} 1 & 0 \\ 0 & 2 \end{matrix} &
   \begin{matrix} 0 \\ 1 \end{matrix}
   \end{array}
   \right]        
   \end{align*}
   
   Thus $c_1 =0$ and $c_2 = 1/2$ so that the solution to the initial
   value problem is given by $$x(t) = \dfrac{te^{t/2}}{2}.$$
   
   -----
   
   \color{black}
   :::



#. Consider the matrix $B = \begin{bmatrix} 5 & -3 & -6 \\ 0 & 2 & 0 \\ 
   3 & -3 & -4 \end{bmatrix}$.

   a. The vector $\mathbf{v} = \begin{bmatrix} 1 \\ -1 \\ 1
      \end{bmatrix}$ is an eigenvector for $B$. What is the
      corresponding eigenvalue?

      **Hint:** Compute the vector $B \mathbf{v}$ and compare with
      $\mathbf{v}$.

   :::{.solution}
   \color{red}
   **Solution:**
      
   
   The product $B \mathbf{v}$ is equal to $$B\mathbf{v} =
   \begin{bmatrix} 2 \\ -2 \\ 2 \end{bmatrix} = 2 \begin{bmatrix} 1 \\
   -1 \\ 1 \end{bmatrix} = 2 \mathbf{v}$$ so the eigenvalue is
   $\lambda = 2$.
   
   -----
   
   \color{black}
   :::


   
   b. Find an eigenvector for $B$ for the eigenvalue $\lambda  = -1$.

   :::{.solution}
   \color{red}
   **Solution:**
   Perform row operations on the matrix $B - (-1)\mathbf{I}_3 = B + \mathbf{I_3}$:
   
   \begin{align*}
   \begin{bmatrix}
   6 & -3 & -6 \\ 0 & 3 & 0 \\ 3 & -3 & -3
   \end{bmatrix}
   \sim 
   \begin{bmatrix}
   2 & -1 & -2 \\ 0 & 1 & 0 \\ 1 & -1 & -1
   \end{bmatrix}   
   \sim 
   \begin{bmatrix}
   0 & 1 & 0 \\ 0 & 1 & 0 \\ 1 & -1 & -1
   \end{bmatrix}      
   \sim 
   \begin{bmatrix}
   1 & -1 & -1 \\ 0 & 1 & 0 \\  0 & 0 & 0 
   \end{bmatrix}         
   \sim 
   \begin{bmatrix}
   1 & 0 & -1 \\ 0 & 1 & 0 \\  0 & 0 & 0 
   \end{bmatrix}            
   \end{align*}
   
   Considering this echelon matrix, we see that an eigenvector for
   $\lambda = -1$ is given by $$\begin{bmatrix} 1 \\ 0 \\ 1
   \end{bmatrix}.$$
   
   -----
   
   \color{black}
   :::



#. Laplace Transforms:

   a. Compute the inverse Laplace tranform $\mathscr{L}^{-1}[F(s)]$ of
      the function $F(s) = \dfrac{3s^2+s+1}{(s+1)(s^2 + 2)}$.

   :::{.solution}
   \color{red}
   **Solution:**
        

   The partial fraction decomposition has the form \begin{equation*}
   \frac{3s^{2}+s+1}{(s+1)(s^{2}+2)}=\frac{A}{s+1}+\frac{Bs+C}{s^{2}+2};
   \end{equation*} combining over a common denominator and matching
   coefficients leads to \begin{equation*} \begin{array}{lrrrcr} s^{2}
   \text{ terms}:&A&+B&&=&3\\ s \text{ terms}:&&B&+C&=&1\\
   \text{constant terms}:&2A&&+C&=&1 \end{array} \end{equation*} We
   can solve the first (respectively, second) equation for $A$
   (respectively, $C$) in terms of $B$: \begin{align*} A&=3-B\\ C&=1-B
   \end{align*} and substituting into the third equation yields
   \begin{align*} (6-2B)+(1-B)&=1\\ -3B&=-6\\ B&=2\\ A&=1\\ C&=-1
   \end{align*} so \begin{equation*}
   \frac{3s^{2}+s+1}{(s+1)(s^{2}+2)}=\frac{1}{s+1}+\frac{2s-1}{s^{2}+2}.
   \end{equation*} Then the inverse transform is \begin{align*}
   {\mathscr{L}}^{-1}\left[\frac{3s^{2}+s+1}{(s+1)(s^{2}+2)}\right]&=%\\
   {\mathscr{L}}^{-1}\left[\frac{1}{s+1}
   \right]+{\mathscr{L}}^{-1}\left[
   \frac{2s}{s^{2}+2}\right]-{\mathscr{L}}^{-1}\left[\frac{1}{s^{2}+2}
   \right]\\ &=e^{-t}+2\cos t\sqrt{2} -\frac{1}{\sqrt{2}}\sin
   t\sqrt{2}.  \end{align*}


   -----
     
   \color{black}
   :::



   b. If $x$ is a solution to $(D^2 + D + 1)x = 1$ with $x(0) = 0$ and
      $x'(0) = 1$, find an expression for $\mathscr{L}[x]$ as a
      function of $s$.


   :::{.solution}
   \color{red}
   **Solution:**

   By the first differentiation formula, applying the Laplace
   Transform to both sides of the problem yields \begin{align*}
   \mathscr{L}{D^{2}x}+\mathscr{L}{Dx}+\mathscr{L}{x}&=\mathscr{L}{1}\\
   \lbrace s^{2}\mathscr{L}{x}-s x(0)-x'(0)\rbrace +\lbrace
   s\mathscr{L}{x}-x(0)\rbrace+\mathscr{L}{x} &=\mathscr{L}{1}\\
   s^{2}\mathscr{L}{x}-1 +s\mathscr{L}{x}+\mathscr{L}{x}
   &=\frac{1}{s}\\
   (s^{2}+s+1)\mathscr{L}{x}&=1+\frac{1}{s}=\frac{1+s}{s}\\
   \mathscr{L}{x}&=\frac{1+s}{s(s^{2}+2+1)} \end{align*}

      
   -----
      
   \color{black}
   :::


#. Let $W = W(h_1(t),h_2(t))$ denote the *Wronskian matrix*
   of the functions $h_1(t) = e^{2t}$ and $h_2(t) = te^{2t}$.  Which
   of the following represents the *determinant* of $W$?
  
   a. $e^{4t}$
   
   b. $(1+4t)e^{4t}$
   
   c. $e^{2t}$
   
   d. $(1+4t)e^{2t}$

   :::{.solution}
   \color{red}
   **Solution:**

   ~~correct answer is c~~ correct answer is a.

   -----
      
   \color{black}
   :::



#. Consider the vectors $\mathbf{v}_1 = \begin{bmatrix} -1
   \\ 1 \\ 0 \\ 0 \end{bmatrix}$, $\mathbf{v}_2 = \begin{bmatrix} -1
   \\ 1 \\ 0 \\ 1 \end{bmatrix}$, and $\mathbf{v}_3 = \begin{bmatrix}
   1 \\ 1 \\ 0 \\ 2\end{bmatrix}$ in $\mathbf{R}^4$, and let $A =
   \begin{bmatrix} -1 & -1 & 1 \\ 1 & 1 & 1 \\ 0 & 0 & 0 \\ 0 & 1 & 2
   \end{bmatrix}$ be the $4 \times 3$ matrix whose columns are the
   $\mathbf{v}_i$. Which of the following statements is correct?
   
   
   a. The vectors $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ are
   *linearly dependent*.

   b. Since $A \sim \begin{bmatrix} 1 & 1 & -1 \\ 0 & 1 & 2 \\ 0 & 0 &
	  2 \\ 0 & 0 & 0 \end{bmatrix}$, the only solution to the equation
	  $A \mathbf{w} = \mathbf{0}$ is $\mathbf{w} = \mathbf{0}$ so the
	  vectors $\mathbf{v}_1,\mathbf{v}_2,\mathbf{v}_3$ are *linearly
	  independent*.

   c. The equation $A \mathbf{w} = \mathbf{x}$ has a solution for
      every vector $\mathbf{x}$ in $\mathbf{R}^4$.

   d. The determinant of $A$ is $\ne 0$.


   :::{.solution}
   \color{red}
   **Solution:**

   The correct response is b. 
   
   Answer a. is incorrect because the vectors are independent.  
   
   Answer c. is incorrect because the given equation has no solution when
   $\mathbf{x} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}$.

   Answer d. is incorrect because the
   determinant of a  $4 \times 3$ (non-square!) matrix is not defined.

   -----
      
   \color{black} 
   :::



#. Let $A$ be an $n \times n$ matrix with constant
   coefficients $a_{ij}$, and let $\mathbf{E}(t)$ be a vector with $n$
   components.  If $\mathbf{v}$ is any vector in $\mathbf{R}^n$, must
   there be a solution $\mathbf{x}(t)$ to the system of equations
   $D\mathbf{x} = A \mathbf{x} + \mathbf{E}(t)$ for which
   $\mathbf{x}(0) = \mathbf{v}$?
   
   a. No, this conclusion is only guaranteed when the system is
     *homogeneous*.
   
   b. No, this conclusion is only guaranteed when the entries of the
      vector $\mathbf{E}(t)$ are *constant* functions of $t$.
   
   c. Yes, this conclusion is the content of the *Existence and
      Uniqueness Theorem for Solutions of Linear Systems*.
   
   d. No, this conclusion is only guaranteed when $\det A \ne 0$.



   :::{.solution}
   \color{red}
   **Solution:**

   correct response is c.
   
   The existence and uniqueness theorem applies for non-homogeneous
   systems (so a is incorrect) and applies so long as the entries of
   $A$ and of $\mathbf{E}$ are *continuous* functions of $t$ (so b. is
   incorrect). Finally, the existence and uniqueness theorem is valid
   even when $A$ has determinant $0$ (so d. is incorrect).

   -----
      
   \color{black} 
   :::

#. Consider the homogeneous system $(\diamondsuit) \quad D
   \mathbf{x} = A \mathbf{x}$ where $A$ is a $3 \times 3$ matrix.
   
   
   a. If $\mathbf{h}(t)$ is a solution, must $\mathbf{h}(0)$ be an
      eigenvector for $A$? Why or why not?

   :::{.solution}
   \color{red}
   **Solution:**

   No, $\mathbf{h}(0)$ need not be an eigenvector. Suppose for example
   that $\mathbf{v}$ and $\mathbf{w}$ are eigenvectors for $A$ with
   eigenvalues $\lambda, \mu$, and suppose that $\lambda \ne \mu$. 
   Then $\mathbf{v} + \mathbf{w}$ is not an eigenvector.
   
   Indeed, since $\lambda \ne \mu$ we know that $\mathbf{v}$ and
   $\mathbf{w}$ are *linearly independent*. Now, for any number
   $\beta$, $$(A-\beta \mathbf{I})(\mathbf{v} + \mathbf{w}) =
   (\lambda - \beta) \mathbf{v} + (\mu - \beta) \mathbf{w}$$ Since
   $\lambda \ne \mu$, at least one of $\lambda - \beta$ or $\mu -
   \beta$ is non-zero, so the linear independence of $\mathbf{v}$ and
   $\mathbf{w}$ shows that $(A-\beta \mathbf{I})(\mathbf{v} +
   \mathbf{w})$ is non-zero. This shows that $\mathbf{v} + \mathbf{w}$
   is not an eigenvector (for *any* eigenvalue $\beta$).
   
   Now, the function $$h(t) = e^{\lambda t} \mathbf{v} + e^{\mu t}
   \mathbf{w}$$ is a solution to $(\diamondsuit)$, and $h(0) =
   \mathbf{v} + \mathbf{w}$.
   
   -----
      
   \color{black} 
   :::




   b. Show that the vectors $\begin{bmatrix} 1 \\ 0 \\
      1\end{bmatrix}$, $\begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}$,
      and $\begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}$ are linearly
      dependent.
   

   :::{.solution}
   \color{red}
   **Solution:**

   We perform row operations on the matrix whose columns are given by
   these vectors: 
   $$
   \begin{bmatrix} 1 & -1 & 1 \\ 0 & 2 & 2 \\ 1 & 1 &
   1 \end{bmatrix} \sim 
   \begin{bmatrix} 1 & -1 & 1 \\ 0 & 2 & 2 \\ 0 & 2 &
   0 \end{bmatrix} 
   \sim 
   \begin{bmatrix} 1 & -1 & 1 \\ 0 & 1 & 0 \\ 0 & 0 &
   1 \end{bmatrix} 
   $$

   Since the resulting echelon matrix has 3 pivots and no free
   variables, the only solution $\mathbf{c}$ to the matrix equation
   $$\begin{bmatrix} 1 & -1 & 1 \\ 0 & 2 & 2 \\ 1 & 1 & 1
   \end{bmatrix} \cdot \begin{bmatrix} c_1 \\ c_2 \\ c_3 \end{bmatrix}
   = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}$$ is $\mathbf{c} =
   \begin{bmatrix} c_1 \\ c_2 \\ c_3 \end{bmatrix} = \mathbf{0}$.
   (Alternatively, you could have obtained this conclusion by noting
   that the *determinant* of the matrix $\begin{bmatrix} 1 & -1 & 1 \\
   0 & 2 & 2 \\ 1 & 1 & 1 \end{bmatrix}$ is equal to $0$).
   
   Thus the only coefficients satisfying the following equation $$c_1
   \begin{bmatrix} 1 \\ 0 \\ 1\end{bmatrix} + c_2 \begin{bmatrix} -1
   \\ 2 \\ 1 \end{bmatrix}
   + c_3\begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix} = \mathbf{0}$$ are
   $c_1 = c_2 = c_3 = 0$; this shows that the vectors are linearly
   independent.
   
   -----
      
   \color{black} 
   :::


   c. Let $\mathbf{h}_1(t),\mathbf{h}_2(t),\mathbf{h}_3(t)$ be
      solutions to $(\diamondsuit)$. Suppose that $\mathbf{h}_1(0) =
      \begin{bmatrix} 1 \\ 0 \\ 1\end{bmatrix}$, $\mathbf{h}_2(0) =
      \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}$, and
      $\mathbf{h}_3(0) = \begin{bmatrix} 1 \\ 2 \\ 1 \end{bmatrix}$
      are the vectors from b. Do the solutions
      $\mathbf{h}_1(t),\mathbf{h}_2(t),\mathbf{h}_3(t)$ generate the
      general solution to $(\diamondsuit)$? Why or why not?

   :::{.solution}
   \color{red}
   **Solution:**

   Yes. Since $A$ is a $3 \times 3$ matrix, one knows that three
   solutions $\mathbf{h}_1(t)$, $\mathbf{h}_2(t)$ and
   $\mathbf{h}_3(t)$ generate the general solution provided that the
   "initial vectors" $\mathbf{h}_1(0),\mathbf{h}_2(0),\mathbf{h}_3(0)$
   are linearly independent; thus the result in part b. shows that the
   $\mathbf{h}_i(t)$ generate the general solution.

   -----
      
   \color{black} 
   :::



#. A drug is absorbed by the body at a rate proportional to
   the amount of the drug present in the bloodstream after $t$ hours. If
   there are $x(t)$ mg of the drug present in the bloodstream at time $t$,
   assume that the drug is absorbed at a rate of $0.5 x(t)$ /hour.
   If a patient receives the drug intravenously at a constant rate of
   3 mg/hour, to which of the following ODEs is $x(t)$ a solution?

   a. $x'(t) = -0.5x(t) + 3$
   
   b. $x'(t) = -0.5x(t); \quad x(0) = 3$
   
   c. $x'(t) = 0.5x(0) + 3$
   
   d. $x'(t) = .5x(t) - 3$


   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response is a.
   
   -----
   
   \color{black}
   :::



#. You are given that a particular solution to
   $$(\heartsuit) \quad (D^2-2D +1)x = e^t$$ is $p(t) = \dfrac{t^2
   e^t}{2}$. Which of the following best represents the general
   solution to $(\heartsuit)$?
   
   a. $c_1 e^t + c_2 te^t$.
   
   b. $\dfrac{t^2 e^t}{2} + c_1 e^t + c_2 te^t$.
   
   c. $\dfrac{t^2 e^t}{2} + c e^t$.
   
   d. $\dfrac{t^2 e^t}{2} + c_1 e^t + c_2 e^{-t}$.
   
   
   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was b.
   
   -----
   
   \color{black}
   :::
   

#. Let $x_1(t)$ and $x_2(t)$ be solutions to the ODE
   $(t+1)x'' + x' + x = 0$. Suppose that $x_1(0) = x_2(0)$ and that
   $x_1'(0) = x_2'(0)$. Which of the following statements must be correct?
   
   a.  $x_1(t) = x_2(t)$ for every $t$.
   
   b. Since the ODE is *normal* on the interval $(-1,\infty)$, we can
      conclude that $x_1(t) = x_2(t)$ for $-1 < t < \infty$.

   c. No conclusion is possible because the existence and uniqueness
      theorem does not apply to this ODE.
	  
   d. We can only conclude that $x_1(t) = x_2(t)$ for all $t$ if we also
      assume that $x''_1(0) = x''_2(0)$.

   :::{.solution}
   \color{red}
   **Solution:**
      
   correct response was b.  Indeed since the equation is of 2nd order
   and since it is normal on the interval $(-1,\infty)$, the
   existence and uniqueness theorem guarantees for any $\alpha,\beta$
   that there is only one solution $x$ which $x(0) = \alpha$ and
   $x'(0) = \beta$.
   
   Assertion a. need not be true since the ODE is not normal on $(-\infty,\infty)$.
   
   And assertion d. is incorrect -- the existence and uniqueness
   theorem doesn't require a condition on the second derivative in
   this case.
   
   -----
   
   \color{black}
   :::
   

#. Show that the functions $$f_1(t) = e^t\cos(t), \quad
   f_2(t) = e^t\sin(t), \quad f_3(t) = e^t$$ are linearly independent.
   
   You have been told that functions like this are independent.
   However, here we want you to demonstrate it directly in this
   case. You may use the *Wronskian test* (with all details needed to
   justify using it) or other, direct arguments from the definition.

   
   :::{.solution}
   \color{red}
   **Solution:**

   There are several possible strategies for solving this problem;
   here we list a few of them.
   
   First, you can use the Wronskian test. This requires computation of
   the first and second derivatives of the $f_i$, which is perhaps
   most easily done using the *exponential shift formula*.
   
   One finds:
   $$D[e^t\cos(t)] = e^t(D+1)[\cos(t)] = e^t(\cos(t) - \sin(t))$$
   $$D[e^t\sin(t)] = e^t(D+1)[\sin(t)] = e^t(\cos(t) + \sin(t))$$
   $$D^2[e^t\cos(t)] = D[e^t(\cos(t) - \sin(t))] = e^t(D+1)[\cos(t) - \sin(t)] = -2e^t \sin(t).$$
   $$D^2[e^t\sin(t)] = D[e^t(\cos(t) + \sin(t))] = e^t(D+1)[\cos(t) + \sin(t)] = 2e^t \cos(t).$$   

   Thus the Wronskian matrix is given by
   $$W = W(f_1,f_2,f_3) = \begin{bmatrix}
   e^t \cos(t) & e^t \sin(t) & e^t \\
   e^t(\cos(t) - \sin(t)) & e^t(\cos(t) + \sin(t)) & e^t \\
   -2e^t\sin(t) & 2e^t \cos(t) & e^t
   \end{bmatrix}$$
   
   Now, according to the Wronskian test, the functions will be
   linearly independent (on the interval $(-\infty,\infty)$) provided that
   $\det W(t_0)$ is non-zero for some $t_0$. If we take $t_0 = 0$, we find that
   
   $$\det W\bigg \vert_{t=0} = \det \begin{bmatrix}
   1 & 0 & 1 \\
   1 & 1 & 1 \\
   0 & 2 & 1
   \end{bmatrix} = \det \begin{bmatrix}
   1 & 1 \\ 
   2 & 1
   \end{bmatrix} + \det \begin{bmatrix}
   1 & 1 \\
   0 & 2
   \end{bmatrix} = -1 + 2 = 1$$
   
   Since this determinant is non-zero, the Wronskian test confirms the
   linear independence of $f_1,f_2,f_3$.

   -----
   
   A second method of solving this problem just uses the *definition of linear independence.*
   
   Suppose that $c_1,c_2,c_3$ are constants and that $$c_1 e^t
   \cos(t) + c_2 e^t \sin(t) + c_2 e^t = 0.$$ To show that the
   functions are linearly independent, we must *argue* that $c_1 = c_2
   = c_3 = 0$.
   
   Factoring out the quantity $e^t$, our assumption shows that
   $$e^t(c_1\cos(t) + c_2\sin(t) + c_3)  = 0.$$
   Since $e^t \ne 0$ for all $t$, we find that
   $$c_1\cos(t) + c_2\sin(t) + c_3  = 0.$$   

   Now, since this equation holds for all times $t$, we may choose
   some particular values of $t$ to find equations for the constants
   $c_i$.
   
   When $t=0$, we find that
   $$0 = c_1 \cos(0) + c_2\sin(0) + c_3 = c_1 + c_3.$$

   When $t=\pi/2$, we find that
   $$0 = c_1 \cos(\pi/2) + c_2\sin(\pi/2) + c_3 = c_2 + c_3.$$

   When $t=\pi$, we find that
   $$0 = c_1 \cos(\pi) + c_2\sin(\pi) + c_3 = -c_1 + c_3.$$

   Now, we solve the system of equations
   \begin{align*}
   0 &= c_1 + c_3 \\
   0 &= c_2 + c_3 \\
   0 &= -c_1 + c_3 \\   
   \end{align*}
   
   Adding the first and third equation gives $0 = 2c_3$ so that $c_3 =
   0$. Now the first equation shows that $c_1 = 0$ and the second
   shows that $c_2 = 0$.
   
   Since we have argued that $c_1=c_2=c_3=0$, we conclude from the
   definition that $f_1,f_2,f_3$ are linearly independent.
   
   
   -----
   
   \color{black}
   :::

   
   
#. Find the Laplace transform of the function
   \begin{equation*}
   f(t)=\left\{ 
   \begin{array}{ll}
   1 & \text{ for } t<1, \\
   t-1 & \text{ for } 1\leq t<2, \\
   1 & \text{ for }t\geq 2.
   \end{array}\right .
   \end{equation*}


   :::{.solution}
   \color{red}
   **Solution:**

   In order to be able to compute the Laplace transform, We first
   rewrite the function $f(t)$ using the *unit step functions*.
   
   We have
   \begin{align*}
   f(t) &= 1 + u_1(t)\cdot(-1 + (t-1)) + u_2(t)\cdot((-(t-1) + 1) \\
   &= 1 + u_1(t)\cdot (t-2) + u_2(t)\cdot(-t +2).
   \end{align*}

   Thus
   \begin{align*}
   \mathscr{L}[f(t)]  &= \mathscr{L}[1 + u_1(t)\cdot (t-2) + u_2(t)\cdot(-t +2)] \\
   &= \mathscr{L}[1] + \mathscr{L}[u_1(t)\cdot (t-2)] + \mathscr{L}[u_2(t)\cdot(-t +2)] \\
   &= \mathscr{L}[1] + e^{-s}\mathscr{L}[(t+1) - 2] + e^{-2s} \mathscr{L}[-(t+2) +2] \\
   &= \mathscr{L}[1] + e^{-s}\mathscr{L}[t-1] + e^{-2s} \mathscr{L}[-t] \\   
   &= (1 - e^{-s})\mathscr{L}[1] +  e^{-s}\mathscr{L}[t] - e^{-2s} \mathscr{L}[t] \\ 
   &= (1 - e^{-s})\mathscr{L}[1] +  (e^{-s} - e^{-2s})\mathscr{L}[t] \\
   &= \dfrac{1 - e^{-s}}{s} + \dfrac{e^{-s} - e^{-2s}}{s^2} \\
   \end{align*}

   -----
   
   \color{black}`
   :::


#. Suppose $g(t)$ is the inverse Laplace transform of 
   \begin{equation*}
   F(s)=\frac{2se^{\pi s/2}}{(s^{2}+4)}.
   \end{equation*}
   Find $g\left(\dfrac{\pi}{4}\right)$.


   :::{.solution}
   \color{red}
   **Solution:**

   We use the *second shift formula* to find $g(t)$. Notice that if we
   set
   $$f(t) = \mathscr{L}^{-1}\left[\dfrac{s}{s^2 + 4}\right] = 
   \cos(2t)$$
   then the second shift formula yields

   \begin{align*}
   g(t) &= \mathscr{L}^{-1}[F(s)] =
   2\mathscr{L}^{-1}\left[e^{(\pi/2)s} \dfrac{s}{s^2+4}\right]  \\
   &= 2 u_{\pi/2}(t)f(t-\pi/2)
   \end{align*}
   
   Thus $u_{\pi/2}(\pi/4) = 0$ so that $g(\pi/4) = 2u_{\pi/2}(\pi/4)
   \cdot f(\pi/4 - \pi/2) = 0$.
   
   -----
   
   \color{black}
   :::
