---
author: George McNinch
title: |
  Math 51 Spring 2022 - Week 8 Wed \
  Row reduction
date: 2022-03-09
bibliography:
 - /home/george/Prof-Math/assets/math-bibliography.bib
 - /home/george/Prof-Math/assets/teaching.bib 
csl: assets/chicago-author-date.csl
header-includes: |
  \usepackage[top=25mm,bottom=25mm,left=25mm,right=25mm]{geometry}
  \usepackage{palatino,mathpazo}
---

This lecture covers material from [@gutermanNitecki, sec. 3.6].


# Row operations and eigenvectors

## Finding eigenvectors: Example 1

::: incremental

- Recall we saw last time that $A = \begin{bmatrix} 1 & 1 \\ 3 & 1
  \end{bmatrix}$ has characteristic polynomial $p(\lambda) =
  \lambda^2 - 2\lambda - 2$, and hence has eigenvalues $\lambda = 1
  \pm \sqrt{3}$.

- Let's find an eigenvector for $\lambda = 1 + \sqrt{3}$. To do so, we
  must solve the matrix equation

  $$\mathbf{0} = (A - (1+ \sqrt{3})\mathbf{I_2})\mathbf{x} =
  \begin{bmatrix} -\sqrt{3} & 1 \\ 3 & -\sqrt{3} \end{bmatrix}
  \mathbf{x}$$

  for $\mathbf{x}$.

- Note that we have row equivalences as follows:
  $$\begin{bmatrix} -\sqrt{3} & 1 \\
  3 & -\sqrt{3} \end{bmatrix} \sim
  \begin{bmatrix} 3 & -\sqrt{3} \\
  3 & -\sqrt{3} \end{bmatrix} \sim
  \begin{bmatrix} 3 & -\sqrt{3} \\
  0 & 0  \end{bmatrix} \sim
  \begin{bmatrix} 1 & -\sqrt{3}/3 \\
  0 & 0  \end{bmatrix}$$

:::

-----

::: incremental

- The echelon matrix  
  $$\begin{bmatrix} 1 & -\sqrt{3}/3 \\ 0 & 0 \end{bmatrix}$$ 
  has one pivot variable $x_1$ and one free variable $x_2$.

- Thus the general solution to $(A - (1+\sqrt{3})\mathbf{I_2})
  \mathbf{x} = \mathbf{0}$ is generated by the vector
  $$\begin{bmatrix}
  \sqrt{3}/3 \\ 1
  \end{bmatrix} \quad \text{or by} \quad
  \mathbf{v} = \begin{bmatrix}
  \sqrt{3} \\ 3
  \end{bmatrix}$$

- In particular $\mathbf{v}$ is a $(1 + \sqrt{3})$-eigenvector for
  $A=\begin{bmatrix} 1 & 1 \\ 3 & 1 \end{bmatrix}$.

:::

-----

Now let's find an eigenvector for $\lambda = 1 - \sqrt{3}$. We must solve:

$$\mathbf{0}  = (A - (1- \sqrt{3})\mathbf{I_2})\mathbf{x} = \begin{bmatrix} \sqrt{3} & 1 \\
3 & \sqrt{3} \end{bmatrix} \mathbf{x}.$$

. . .

We have row equivalences as follows:
$$\begin{bmatrix} \sqrt{3} & 1 \\
3 & \sqrt{3} \end{bmatrix} \sim
\begin{bmatrix} 3 & \sqrt{3} \\
3 & \sqrt{3} \end{bmatrix} \sim
\begin{bmatrix} 3 & \sqrt{3} \\
0 & 0  \end{bmatrix} \sim
\begin{bmatrix} 1 & \sqrt{3}/3 \\
0 & 0  \end{bmatrix}$$

. . .

Again there is only one free variable, so the general solution to
$\mathbf{0} = (A - (1- \sqrt{3})\mathbf{I_2})\mathbf{x}$ is generated by

$$\mathbf{w} = \begin{bmatrix}
 -\sqrt{3} \\ 3
\end{bmatrix}$$

. . .

and this vector $\mathbf{w}$ is a $(1 - \sqrt{3})$-eigenvector for $A$.

## Finding eigenvectors: Example 2

We saw earlier that the eigenvalues of $B = \begin{bmatrix} 0 & -2 & 2 \\ 
1 & 3 & -2 \\ 2 & 4 & -3 \end{bmatrix}$ are

$$\lambda=1 \quad \text{with multiplicity 2, and} \quad \lambda = -2 \quad \text{(with multiplicity 1)}.$$

. . .

Let's find the eigenvector(s) for $\lambda  =1$. We need to solve the equation

$$\mathbf{0} = (B - \mathbf{I_3})\mathbf{x} = \left(\begin{bmatrix} 0 & -2 & 2 \\ 
1 & 3 & -2 \\ 2 & 4 & -3 \end{bmatrix} - \begin{bmatrix}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
\end{bmatrix}\right) \mathbf{x}=
\begin{bmatrix} -1 & -2 & 2 \\ 
1 & 2 & -2  \\ 2 & 4 & -4 \end{bmatrix} \mathbf{x}$$ 


-----

We have row equivalences
$$\begin{bmatrix} -1 & -2 & 2 \\ 
1 & 2 & -2  \\ 2 & 4 & -4 \end{bmatrix} \sim 
\begin{bmatrix} 1 & 2 & -2 \\ 
1 & 2 & -2  \\ 1 & 2 & -2 \end{bmatrix} \sim
\begin{bmatrix} 1 & 2 & -2 \\ 
0 & 0  & 0  \\ 0 & 0 & 0  \end{bmatrix}$$

. . .

In this case, there are two free variables, $x_2$ and $x_3$
Note that $0 = x_1 + 2x_2 - 2x_3.$

. . .

We find solutions

$$\mathbf{u} = \begin{bmatrix} -2 \\ 1 \\ 0 \end{bmatrix} 
\quad
\mathbf{v} =\begin{bmatrix} 2 \\ 0 \\ 1 \end{bmatrix}$$

The vectors $\mathbf{u}$ and $\mathbf{v}$ 
generate the $1$-eigenspace of the matrix $B$.


-----

Finally, let us find eigenvector(s) for $B$ for $\lambda=-2$:

We need to solve the equation
$$\mathbf{0} = (B + 2\mathbf{I_3})\mathbf{x} = \left(\begin{bmatrix} 0 & -2 & 2 \\ 
1 & 3 & -2 \\ 2 & 4 & -3 \end{bmatrix} + \begin{bmatrix}
2 & 0 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 2
\end{bmatrix}\right) \mathbf{x}=
\begin{bmatrix} 2 & -2 & 2 \\ 
1 & 5 & -2  \\ 2 & 4 & -1 \end{bmatrix} \mathbf{x}$$ 

. . .

We have row equivalences
\begin{align*}
&\begin{bmatrix} 2 & -2 & 2 \\ 
1 & 5 & -2  \\ 2 & 4 & -1 \end{bmatrix} \sim
\begin{bmatrix} 1 & -1 & 1 \\ 
1 & 5 & -2  \\ 2 & 4 & -1 \end{bmatrix} \sim
\begin{bmatrix} 1 & -1 & 1 \\ 
 0 & 6 & -3 \\
 2 & 4  & -1\end{bmatrix} \sim 
\begin{bmatrix} 1 & -1 & 1 \\ 
 0 & 2 & -1 \\
 0 & 6 & -3 \end{bmatrix}   \\
\sim & \begin{bmatrix} 1 & -1 & 1 \\ 
 0 & 2& -1 \\
 0 & 0 & 0 \end{bmatrix} 
\end{align*}

----

We have one free variable, $x_3$. Equating coefficients in the expression
$\mathbf{0} = B \mathbf{x}$ gives
$x_2 = x_3/2$ and $x_1 - x_2 + x_3 =0$, so that
$x_1 = x_2 - x_3 = -x_3/2$.

. . .

Setting $x_3 =1$ gives $x_2 = 1/2$ and $x_1 = -1/2$.

Thus
$$\begin{bmatrix}
-1/2 \\ 1/2 \\ 1
\end{bmatrix}$$
is a $(-2)$-eigenvector for $B$.

. . .

One might prefer to replace  this vector by
$\mathbf{w} = \begin{bmatrix}
-1 \\ 1 \\ 2
\end{bmatrix}$.

The vector $\mathbf{w}$ generates the $(-2)$-eigenspace of $B$.


## Linearly independence and eigenvectors

::: incremental

- Consider a homogeneous system of ODES $(\clubsuit) \quad D \mathbf{x} = A \mathbf{x}$. 

- for any eigenvector $\mathbf{v}$ of $A$ with *real* eigenvalue $\lambda$, we are going to 
  find a solution $\mathbf{h}(t)$ of $(\clubsuit)$ for which
  $\mathbf{h}(0) = \mathbf{v}$.
  
- so in order to find solutions $\mathbf{h}_1,\dots,\mathbf{h}_n$ of
  $(\clubsuit)$ with linearly independent initial vectors
  $\mathbf{h}_1(t_0),\dots,\mathbf{h}_n(t_0)$, a natural first
  question is this: under what conditions does an $n \times n$ matrix
  $A$ have $n$ linearly independent eigenvectors?

:::

## Matrices with $n$ distinct eigenvalues.

Let $A$ be an $n \times n$ matrix.

::: incremental

- if $A$ has $n$ *distinct* eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$
  choose an eigenvector $\mathbf{v_i}$ for each $\lambda_i$. \
  
- **Fact:** with these assumptions, the vectors
  $\mathbf{v_1},\dots,\mathbf{v_n}$ are always linearly independent.

:::

## Example

::: incremental

- let $A = \begin{bmatrix}
  3 & 1 \\
  -2 & 0
  \end{bmatrix}$

- the char. poly is $p(\lambda) = -\lambda(3-\lambda) + 2 =
  \lambda^2 - 3\lambda + 2 = (\lambda -2)(\lambda-1)$. 
  
- thus $A$ has eigenvalues $\lambda = 1,2$. In particular, it has two distinct eigenvalues.

- to find a $1$-eigenvector for $A$, we must consider the matrix
  equation $(A-\mathbf{I_2})\mathbf{x} = \mathbf{0}$.
  
- We have $A - \mathbf{I_2} = \begin{bmatrix} 2 & 1 \\ -2 & -1 \end{bmatrix}
  \sim \begin{bmatrix} 2 & 1 \\ 0 & 0 \end{bmatrix}$.
  
- Thus a $1$-eigenvector is $\begin{bmatrix} -1/2 \\ 1 \end{bmatrix}$ or
  $\mathbf{v} = \begin{bmatrix} -1 \\ 2 \end{bmatrix}$.

:::

-----

::: incremental

- to find a $2$-eigenvector for $A$ we must consider the eqn $(A -
  2\mathbf{I_2}) \mathbf{x} = \mathbf{0}$.

- we have $(A - 2\mathbf{I_2}) = \begin{bmatrix}1 & 1 \\ -2 & -2 
  \end{bmatrix} \sim \begin{bmatrix}1 & 1 \\ 0 & 0  
  \end{bmatrix}$.

- thus a $2$-eigenvector is $\mathbf{w} = \begin{bmatrix} -1 \\ 1
  \end{bmatrix}$.

- now the fact asserted earlier is that 
  $\mathbf{v}$ and $\mathbf{w}$ are *linearly independent*.
  
- recall that   $$\mathbf{v} = \begin{bmatrix} -1 \\ 2 \end{bmatrix},
\quad \mathbf{w} = \begin{bmatrix} -1 \\ 1
  \end{bmatrix}.$$

- we could of course just check this linearly independence directly.
  since $$\det \begin{bmatrix} -1 & -1 \\ 2 & 1 \end{bmatrix} = -1 +2
  =1 \ne 0$$ the vectors $\mathbf{v}$ and $\mathbf{w}$ are indeed
  linearly independent.  
  
:::

-----

::: incremental

- Here is another argument for that linear independence. Suppose that
  $$\alpha \mathbf{v} + \beta \mathbf{w} = \mathbf{0}$$ for scalars
  $\alpha,\beta$. Then $\mathbf{0} = (A - 2I)(\alpha \mathbf{v} +
  \beta \mathbf{w})$

- Now, $(A-2I)\mathbf{w} = 0$ since $\mathbf{w}$ is a $2$-eigenvector; thus
  we find that
  \begin{align*}
  \mathbf{0} &= (A - 2I)(\alpha \mathbf{v} + \beta \mathbf{w}) = \alpha(A - 2I)(\mathbf{v}) \\
  &= \alpha \begin{bmatrix}1 & 1 \\ -2 & -2 
  \end{bmatrix} \begin{bmatrix} -1 \\ 2 \end{bmatrix}
   = \alpha \begin{bmatrix} 1 \\ -2 \end{bmatrix}
  \end{align*}
  which shows that $\alpha = 0$.
  
- Similarly $(A-I)\mathbf{v} = 0$ since $\mathbf{v}$ is a $1$-eigenvector; thus
  we find that
  \begin{align*}
  \mathbf{0} &= (A - I)(\alpha \mathbf{v} + \beta \mathbf{w}) = \beta(A - 2I)(\mathbf{w}) \\
  &= \beta 
   \begin{bmatrix} 2 & 1 \\ -2 & -1 \end{bmatrix}
   \begin{bmatrix} -1 \\ 1
  \end{bmatrix}
  = \beta \begin{bmatrix}
  -1 \\ 1
  \end{bmatrix}
  \end{align*}
  which shows that $\beta = 0$.
  

:::

## Solving non-homogeneous matrix equations via row operations

::: incremental

- let $B$ be an $m \times n$ matrix, and let $\mathbf{v} =
  \begin{bmatrix} v_1 \\ \vdots \\ v_m \end{bmatrix}$ in
  $\mathbb{R}^m$.
  
- we want to solve the matrix equation 
  $$B \mathbf{x} = \mathbf{v}$$
  for $\mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}$ in $\mathbb{R}^n$

:::


## Bibliography {.unnumbered}

::: {.refs}
:::
