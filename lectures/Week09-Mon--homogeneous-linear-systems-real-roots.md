---
author: George McNinch
title: |
  Math 51 Spring 2022 - Week 9 Mon \
  Homogeneous linear systems (real roots)
date: 2022-03-14
bibliography:
 - /home/george/Prof-Math/assets/math-bibliography.bib
 - /home/george/Prof-Math/assets/teaching.bib 
csl: assets/chicago-author-date.csl
header-includes: |
  \usepackage[top=25mm,bottom=25mm,left=25mm,right=25mm]{geometry}
  \usepackage{palatino,mathpazo}
---




# Homogeneous systems: real roots

This lecture covers material from [@gutermanNitecki, sec. 3.7].

Given an $n \times n$ matrix $A$ and an eigenvalue $\lambda$ of $A$,
we are going to refer to the *solution space* of the homogeneous
equation $(A - \lambda \mathbf{I_n})\mathbf{x} = \mathbf{0}$ as the $\lambda$-eigenspace of
$A$.

The non-zero vectors in this solution space are precisely the
$\lambda$-eigenvectors of $A$.


# Solutions from eigenvalues/eigenvectors

::: incremental

- Consider a homogeneous system of linear ODEs *with constant coefficients*
  $$(\clubsuit) \quad D \mathbf{x} = A \mathbf{x}$$
  where $A$ is an $n\times n$ matrix.

- If the *real number* $\lambda$ is an eigenvalue of $A$ and if
  $\mathbf{v}$ is a corresponding eigenvector, then the vector-valued
  function $$\mathbf{h}(t) = e^{\lambda t} \mathbf{v}$$ is a
  *solution* to $(\clubsuit)$.

::: 

#

Indeed, e.g. $\mathbf{h}(t) = e^{\lambda
t}\mathbf{v} = e^{\lambda t} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\  v_n
\end{bmatrix} = \begin{bmatrix} v_1 e^{\lambda t} \\ v_2 e^{\lambda t}
\\ \vdots \\ v_n e^{\lambda t} \end{bmatrix}.$

. . .

Then differentiation gives $D[\mathbf{h}(t)] = \lambda e^{\lambda t}
\mathbf{v}$.

. . .

While the fact that $\mathbf{v}$ is an eigenvector gives
$A \mathbf{h}(t) = e^{\lambda t} A \mathbf{v} = \lambda e^{\lambda t}
\mathbf{v}.$

# Towards the general solution

Supose that there are $n$ *linearly independent* eigenvectors
$\mathbf{v_1},\mathbf{v_2}, \cdots, \mathbf{v_n}$ for the matrix $A$.

. . .

Set $\mathbf{h_i}(t) = e^{\lambda_i t} \mathbf{v_i}$ for $1
\le i \le n$.

. . .

Since $\mathbf{h_i}(0) = \mathbf{v_i}$ and since the $\mathbf{v_i}$
are linearly independent, we see that the $\mathbf{h_i}$ generate the
general solution to the homogeneous equation $(*) \quad D \mathbf{x} = A \mathbf{x}$;

. . .

i.e. the general solution to $(*)$ is
$$\mathbf{x}(t) = c_1 \mathbf{h_1}(t) + c_2 \mathbf{h_2}(t) + \cdots + c_n \mathbf{h_n}(t).$$

. . .

- So a natural first question is this: under what conditions does an
  $n\times n$ matrix $A$ have $n$ linearly independent eigenvectors?

# Matrices with $n$ distinct eigenvalues.

Let $A$ be an $n \times n$ matrix.

::: incremental

- if $A$ has $n$ *distinct* eigenvalues $\lambda_1,\lambda_2,\dots,\lambda_n$
  choose an eigenvector $\mathbf{v_i}$ for each $\lambda_i$. \
  
- We observed already last week that under these assumptions, the
  vectors $\mathbf{v_1},\dots,\mathbf{v_n}$ are always linearly
  independent.

:::


# The distinct eigenvalues of a square matrix

::: incremental

- let $A$ be an $n \times n$ matrix.

- Suppose that $\lambda_1,\dots,\lambda_m$ are the distinct
  eigenvalues of $A$ (each of which may have some multiplicity). 
  
  Note that $m \le n$.

- For each $i$, find linearly independent vectors which generate the
  $\lambda_i$-eigenspace 
  
  (i.e. the solution space to $(A -\lambda_i \mathbf{I_n})\mathbf{x} =
  \mathbf{0}$).
  
- **Fact**: the full list of vectors found in this way will be
  linearly independent.

- **Fact**: if $\lambda$ has multiplicity $d$ as root of the
  characteristic polynomial $p(\lambda)$, then the solution space
  to $(A - \lambda \mathbf{I_n}) \mathbf{x} = \mathbf{0}$ is generated
  by no more than $d$ linearly independent vectors.

:::

# Example

::: incremental

- Suppose the $3 \times 3$ matrix has two eigenvalues $\lambda = 2$
  and $\lambda = -3$, where $\lambda = -3$ has multiplicity two.

- since $\lambda=2$ has multiplicity one, the $2$-eigenspace of $A$ --
  i.e. the solution space to $(A - 2\mathbf{I_2})\mathbf{x} =
  \mathbf{0}$ -- is generated by a single vector $\mathbf{v}$.

- since $\lambda=-3$ has multiplicity two, the $(-3)$-eigenspace is
  generated by *at most two* linearly independent vectors.

- *if* the $(-3)$-eigenspace is generated by two linearly indep
  vectors $\mathbf{w}$ and $\mathbf{u}$, then the three vectors
  $\mathbf{v},\mathbf{w},\mathbf{u}$ are linearly indep and
  
  $$\mathbf{h_1}(t) = e^{2t} \mathbf{v}, \quad \mathbf{h_2}(t) = 
  e^{-3t}\mathbf{w}, \quad \mathbf{h_3}(t) = e^{-3t}\mathbf{u}$$
  
  generate the general solution to the system of ODEs $D \mathbf{x} = A \mathbf{x}$.

:::

#

::: incremental

- if the $(-3)$-eigenspace is generated by only one vector
  $\mathbf{w}$, then we know how to find two solutions to the system
  of ODEs $D \mathbf{x} = A \mathbf{x}$, namely
  
  $$\mathbf{h_1}(t) = e^{2t} \mathbf{v}, \quad  \mathbf{h_2}(t) = 
  e^{-3t}\mathbf{w}$$
  
- but we not yet seen how to find the general solution to this system
  (there are no other eigenvectors available, so we will need an
  alternate way to construct a solution -- we'll return to this
  later on!)
  

:::

# Example

::: incremental

- let $A = \begin{bmatrix} 1 & 2 & 0 \\ 0 & 2 & 0 \\ 0 & 0 & 1
  \end{bmatrix}$.
  
- the characteristic polynomial is $p(\lambda) = (1-\lambda)^2(2-\lambda)$.

- the eigenvalue $\lambda = 2$ has multiplicity one; to find an eigenvector we study
  the matrix $A - 2\mathbf{I_3} = \begin{bmatrix} -1 & 2 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & -1
  \end{bmatrix} \sim 
  \begin{bmatrix} 1 & -2 & 0 \\  0 & 0 & 1\\  0 & 0 & 0 
  \end{bmatrix}$

- The equation $(A - 2\mathbf{I_3})\mathbf{x} = \mathbf{0}$ has solution generated
  by $\mathbf{v} = \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}.$
  
- so $\mathbf{v}$ is a $2$-eigenvector

:::

#

::: incremental

- The eigenvalue $\lambda = 1$ has multiplicity 2. So before we start,
  we know we will either find 1 or 2 linearly independent
  $1$-eigenvectors which generate the $1$-eigenspace.
  
- To find the generators, we consider the matrix
  $$A - \mathbf{I_3} = \begin{bmatrix} 0 & 2 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}
  \sim \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$

- Looking for solutions to the matrix equation $(A - \mathbf{I_3}) \mathbf{x} = \mathbf{0}$, we see
  that $x_2$ is a "pivot variable" and $x_1,x_3$ are free variables. We thus find two linearly independent
  $1$-eigenvectors
  $$\mathbf{w} = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix} \quad \text{and } \quad 
  \mathbf{u} = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}.$$
  

:::

#

::: incremental

- now the **Fact** from a previous slide tells us that $\mathbf{v},
  \mathbf{w}, \mathbf{u}$ are linearly independent.
  
- in particular, the general solution to the system of ODEs
  $D\mathbf{x} = A \mathbf{x}$ is given by
  
  $$\mathbf{x}(t) = c_1 e^{2t}\mathbf{v} + c_2 e^t\mathbf{w} + c_3 e^t \mathbf{u}
  = c_1 e^{2t} \begin{bmatrix} 2 \\ 1 \\ 0 \end{bmatrix}
  + c_2 e^t \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
  + c_3 e^t \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$$
   

:::

# An example we can't yet solve

::: incremental

- Let $A = \begin{bmatrix} 1 & 0 \\ 1 & 1 \end{bmatrix}$.

- the characteristic polynomial is $(1-\lambda)^2$, and the only
  eigenvalue is $\lambda = 1$ (with multiplicity two).
  
- the $1$-eigenspace is generated by the single vector
  $\mathbf{v} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}$
  
- $\mathbf{h}(t) = e^t \mathbf{v} = \begin{bmatrix} 0 \\ e^t
  \end{bmatrix}$ is a *solution* to $D \mathbf{x} = A \mathbf{x}$, but
  by itself it does not generate the general solution

:::

## Initial-value problems in this context

::: incremental

- Consider a homogeneous system $(\clubsuit) \quad D \mathbf{x} = A \mathbf{x}$
  where $A$ is an $n \times n$ matrix.
  
- Recall that the *existence and uniqueness Theorem* says that for any
  vector $\mathbf{v}$ in $\mathbb{R}^n$, there is a solution $\mathbf{}$ to $(\clubsuit)$
  with $\mathbf{x}(t_0) = \mathbf{v}$.

- to find this solution $\mathbf{x}$, first generators
  $\mathbf{h}_1,\dots,\mathbf{h}_n$ for find the *general solution* to $(\clubsuit)$:
  
  $$c_1 \mathbf{h}_1 + c_2 \mathbf{h}_2 + \cdots + c_n \mathbf{h}_n$$

- we now must find $c_1,\dots,c_n$ so that 

  $$c_1 \mathbf{h}_1(t_0) + c_2 \mathbf{h}_2(t_0)+ \cdots + c_n
  \mathbf{h}_n(t_0)$$
  
:::

-----

::: incremental 
  
- if $B$ denotes the $n \times n$ matrix whose columns are the vectors 
  $\mathbf{h}_1(t_0),\dots,\mathbf{h}_n(t_0)$, we can find the $c_i$ by solving the matrix
  equations
  $$B \mathbf{c} = \mathbf{v}$$
  for $\mathbf{c} = \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_n \end{bmatrix}$

:::

## Example

::: incremental

- Let $A = \begin{bmatrix} 1 & 1 & 2 \\ 0 & 2 & 0 \\ 0 & 0 & -1
  \end{bmatrix} \mathbf{x}$ and solve the IVP $$D \mathbf{x} = A
  \mathbf{x}, \quad \mathbf{x}(0) = \begin{bmatrix} 1\\ 1 \\ 1
  \end{bmatrix}.$$

- since the coefficient matrix $A$ is upper triangular, its
  eigenvalues are the diagonal entries. So we need to find eigenvectors
  for eigenvalues $\lambda = 1,2,-1$.

- $\lambda = 1$. 
  $$A - I = \begin{bmatrix} 1 & 1 & 2 \\ 0 & 2 & 0 \\ 0 & 0 & -1 \end{bmatrix} - 
  \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
  =  \begin{bmatrix} 0 & 1 & 2 \\ 0 & 1 & 0 \\ 0 & 0 & -2 \end{bmatrix}
  \sim \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}$$

:::

----

::: incremental

- 
  $$A - I \sim \begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}$$
  
  Thus an eigenvector for $\lambda = 1$ is $\mathbf{w}_1 =
  \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}$.

- $\lambda = 2$: 
  $$A-2I = \begin{bmatrix} -1 & 1 & 2 \\ 0 & 0 & 0 \\ 0 & 0 & -3 \end{bmatrix}
  \sim \begin{bmatrix} 1 & -1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}$$

  Thus an eigenvector for $\lambda = 2$ is $\mathbf{w}_2 = 
  \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}.$

:::

----

::: incremental

- $\lambda = -1$:
  $$A+I = \begin{bmatrix} 2 & 1 & 2 \\ 0 & 3 & 0 \\ 0 & 0 & 0 \end{bmatrix}
  \sim \begin{bmatrix} 1 & 0 & 1 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$$

  Thus an eigenvector for $\lambda = -1$ is $\mathbf{w}_3 = 
  \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}.$


- we now find that the *general solution* to $D\mathbf{x} = A
  \mathbf{x}$ is $$c_1 e^t \mathbf{w}_1 + c_2 e^{2t} \mathbf{w}_2 +
  c_3 e^{-t} \mathbf{w}_3 = c_1 e^t \begin{bmatrix} 1 \\ 0 \\ 0
  \end{bmatrix}
  + c_2 e^{2t} \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
  + c_3 e^{-t} \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$$


- let 
  $B = \begin{bmatrix} 1 & -1 & -1 \\
  1 & 1 & 0 \\
  0 & 0 & 1 \end{bmatrix}$

:::

----

::: incremental

- let 
  $B = \begin{bmatrix} 1 & -1 & -1 \\
  1 & 1 & 0 \\
  0 & 0 & 1 \end{bmatrix}$

- to solve the initial value problem $(\clubsuit)$, we need to solve the matrix equation
  $$B \begin{bmatrix} c_1 \\ c_2 \\ c_3 \end{bmatrix} = \begin{bmatrix} 1\\ 1 \\ 1
  \end{bmatrix}$$
  
- so we need to do row operations on the augmented matrix
  $\left [
  \begin{array}{l|l}
  B & \begin{matrix} 1 \\ 1 \\ 1
  \end{matrix}
  \end{array} \right]$

- 
  $$\left [
  \begin{array}{l|l}
  \begin{matrix} 1 & -1 & -1 \\
  1 & 1 & 0 \\
  0 & 0 & 1 \end{matrix} 
  & \begin{matrix} 1 \\ 1 \\ 1
  \end{matrix}
  \end{array} \right]
  \sim \left [
  \begin{array}{l|l}
  \begin{matrix} 1 & -1 & -1 \\
  0 & 2 & 1 \\
  0 & 0 & 1 \end{matrix} 
  & \begin{matrix} 1 \\ 0 \\ 1
  \end{matrix}
  \end{array} \right]
  \sim \left [
  \begin{array}{l|l}
  \begin{matrix} 1 & -1 & 0 \\
  0 & 2 & 0 \\
  0 & 0 & 1 \end{matrix} 
  & \begin{matrix} 2 \\ -1 \\ 1
  \end{matrix}
  \end{array} \right]
  \sim \left [
  \begin{array}{l|l}
  \begin{matrix} 1 & 0 & 0 \\
  0 & 1 & 0 \\
  0 & 0 & 1 \end{matrix} 
  & \begin{matrix} 3/2 \\ -1/2 \\ 1
  \end{matrix}
  \end{array} \right]$$

:::

---

::: incremental

- thus we find that $\mathbf{c} = \begin{bmatrix} 3/2 \\ -1/2 \\ 1
  \end{bmatrix}$
  
- so that the solution to the IVP $(\clubsuit)$ is given by  
  $$\mathbf{x}(t) = (3/2) e^t \begin{bmatrix} 1 \\ 0 \\ 0
  \end{bmatrix}
  + (-1/2) e^{2t} \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
  +  e^{-t} \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}$$

- let's check the initial condition:
  $$\mathbf{x}(0) = (3/2) \begin{bmatrix} 1 \\ 1 \\ 0
  \end{bmatrix}
  + (-1/2) \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
  +  \begin{bmatrix} -1 \\ 0 \\ 1 \end{bmatrix}
  = \begin{bmatrix} 3/2 + 1/2 - 1 \\ 3/2 - 1/2 \\ 1 \end{bmatrix}
  = \begin{bmatrix} 1 \\ 1 \\ 1 \end{bmatrix}$$

:::

# Bibliography {.unnumbered}

::: {.refs}
:::
